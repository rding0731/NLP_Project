from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


tf = TfidfVectorizer(ngram_range = (1,1), max_df = 0.95, min_df = 0, 
                         stop_words = Stopwords)

tf_idf = tf.fit_transform(Step4)

vocab = np.array(tf.get_feature_names())

## OneHot Encoding 
## This is to include the detailed risk as additional features in the sparse matrix.  
additional = pd.get_dummies(process["Detailed \nRisk"]).as_matrix()

## OneHot Encoding 
## This is to include the detailed risk as additional features in the sparse matrix.  
additional = pd.get_dummies(process["Detailed \nRisk"]).as_matrix()

from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
# connectivity = kneighbors_graph(tf_idf, n_neighbors = 10, include_self = False)

ward = AgglomerativeClustering(n_clusters = 100, linkage = 'ward').fit(total_matrix.toarray())

label = ward.labels_

////////////////////////////////////////////
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def print_topic_words(Text, num_topics = 100, num_top_words = 10, max_iteration = 10):
       
    tf = TfidfVectorizer(ngram_range = (1,1), max_df = 0.95, min_df = 0, 
                         stop_words = Stopwords)
    
    tf_idf = tf.fit_transform(Text)
    vocab = np.array(tf.get_feature_names())
    
    clf = LatentDirichletAllocation(n_topics=num_topics
                                    , max_iter= max_iteration
                                    , learning_method = 'online'
                                    # The online method is suited for the largedata
                                    , learning_offset = 50.
                                    , total_samples = 1e6
                                   # , n_jobs = -1
                                    , random_state=1)
    doctopic = clf.fit_transform(tf_idf)
    
    topic_words = []
    for topic in clf.components_:
        word_idx = np.argsort(topic)[::-1][0:num_top_words]
        topic_words.append([vocab[i] for i in word_idx])
    
    return doctopic, topic_words

## user input
num_topics = 100
num_top_words = 15
max_iteration = 10

doctopic, topic_words = print_topic_words(Step3, num_topics, num_top_words, max_iteration)

reordered = []
doctopic_weights = np.sum(doctopic, axis=0)
ranking = np.argsort(doctopic_weights)[::-1]  

for i, x in enumerate(ranking):
    reordered.append(topic_words[x])   
    print("Topic {}: {}".format(ranking[i], ' ,'.join(reordered[i][: num_top_words])))

data = doctopic/ (np.sum(doctopic, axis = 1, keepdims = True))

added = pd.DataFrame(data,columns = range(num_topics))
